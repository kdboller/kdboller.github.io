---
layout: post
title:  "Scaling Analytical Insights with Python (Part 1)"
date:   2017-07-09 12:00:00 -0500
categories: 
---

<img src="/assets/octocat-love-chompy-5.png" alt="Octocat Loves Chompy" height="400"  style="width: 100%">

<!--<h1><strong>Please note that this post is still under development but a significant amount has now been completed.</strong></h1>-->

<p>
    In recent months, I’ve written about some of the critical undertakings and initiatives which I oversee as VP of Product at FloSports.  
  These have included my efforts to build a data informed culture through product experimentation, our overall approach to our analytics tech stack, 
  and our approach to building and reviewing our rolling financial forecasts.
</p>

<p>
    I’ve also mentioned that the implementation of our data warehouse and the use of data analytics software, including Periscope Data, Segment, 
  and Mode Analytics, have been fairly transformative across the company.  And, as business intelligence and data analysis requests and 
  feedback grow within an organization, it is critical to put processes and analytical procedures in place that reduce the queuing of data requests
  -- fast-paced and efficient approaches significantly reduce the time gathering the data and increase the level of insight derived from the data.
</p>

<p><i>In other words, spend less time calculating your LTV and more time focusing on efforts to grow your LTV. </i></p>

<p>
<strong><u>Here's what I will cover in this three part post series:</u></strong>
<ul>
    <li><strong><u>Part 1.</u></strong>  Cohort retention analysis in Python; I discovered this over a year ago on Greg Reda’s blog, and it was remarkably helpful</li>
    <ul><li>Taking this an additional step further and calculating weighted average retention for monthly cohorts</li>
    <li>This can then become a single series fed directly into a financial model</li></ul>
  <li><strong><u>Part 2.</u></strong>  The power of Mode Analytics, which combines SQL and Python into a single web application</li>
    <ul>
      <li>Outside of Mode, I use Jupyter Notebook for all analysis in Python</li>
      <li>Mode Analytics has a powerful offering for Python, which is completely self-contained within their overall reporting application</li>
    </ul> 
  <li><strong><u>Part 3.</u></strong>  The use of Python, in place of Excel, to conduct large scale financial and operational analysis; the analysis / dataframes can ultimately, as the last step, be pasted into a financial model</li>
      <ul><li>The best resource I’ve found for Python business application to-date is <a href="http://pbpython.com/" target="_blank">Chris Moffit’s Practical Business Python blog</a></li>
      </ul>
</ul>


</p>
<strong><u>Cohort Retention Analysis with Python</u></strong>
<p>
Rather than reconstruct Greg Reda’s remarkably helpful post, which can be found <a href="http://www.gregreda.com/2015/08/23/cohort-analysis-with-python/" target="_blank">here</a>, I will simply continue from where he leaves off by showing how to calculate
M1, M2, etc. weighted average retention.</p>

<p>
As mentioned in <a href="http://andrewchen.co/the-easiest-spreadsheet-for-churn-mrr-and-cohort-analysis-guest-post/" target="_blank">this guest post on Andrew Chen’s blog</a>, Christoph Janz has written some of the most helpful essays on SaaS metrics and cohort analyses.
One of the screenshots in Christoph’s guest post, from his model, shows the calculated weighted average retention by cohort month.
</p>

<p>
Here’s a screenshot from that guest post of what this looks like (note the fifth footnote):
<img src="/assets/ChristophJanz_CohortAnalysisNotes.png" alt="Janz Cohort Analyses Screenshot" height="500"  style="width: 100%">

</p>

<p>
I certainly believe Christoph's model and overall helpfulness to SaaS companies is fairly outstanding.  However, as a company scales, there are challenges with activities such as pasting cohort data into a model and then calculating weighted average retention, and other metrics, within Excel.
</p>
<p>
<strong>At FloSports, I am constantly thinking about and looking to create scaleable solutions to deal with some of the challenges which we've run into as our data sources and volume of data increases.</strong> Some examples of these challenges which I must always consider include:
<ul>
    <li>We have 25+ verticals with multiple subscription offerings for each.</li>
    <li>New data is constantly being generated by subscribers, including experimentation data, and we need to be able to quickly evaluate all of this multiple ways in order to "cut our losers quickly and let our winners run."</li>
    <li>Our verticals are at different stages within their lifecycles, and building and updating flexible subscriber waterfalls in Excel, as one example, can be rather time consuming.</li>
</ul></p>

<strong><u>Calculating Weighted Average Subscriber Retention with Python</u></strong>
<p>
If you would like to follow along with this explanation in Jupyter notebook, you will just need to use Greg Reda’s code from his post (<a href="http://www.gregreda.com/2015/08/23/cohort-analysis-with-python/" target="_blank">here's the link again</a>), in order to
arrive at my starting point -- I’m starting after his last code snippet, which uses Seaborn and generates a heat map.
</p>

<p>
    In his post, Greg used the unstack method in order to create a matrix where each column is the Cohort Group and each row is the Cohort Period.
    
Using this unstack approach and then resetting the index, we have a flattened dataframe which we can now manipulate in order to calculate the weighted average retention by Cohort Period, e.g., Month 1.  Below the code block is what the output for this dataframe looks like in Jupyter Notebook.
</p>

```python
# Unstack the TotalUsers
unstacked = cohorts['TotalUsers'].unstack(0)
unstacked.reset_index()
```

<img src="/assets/unstacked_df.png" alt="Unstacked Cohorts Dataframe" height="400"  style="width: 100%">

<p>We have successfully manipulated our subscriber data in order to create Cohort Groups and Cohort Periods for those groups, in large part thanks to Greg; and now we can create a separate dataframe which will contain our weighted average retentions across these combined cohorts for each of their monthly periods for which we have data.</p>

<p>Below, we essentially create a dataframe with a reset index, calculate the sums across all rows (which represent each Cohort Period), calculate weighted percentages by dividing by our first row of Total Users, and then transpose the weighted_average dataframe to become a new dataframe; this dataframe has Total Users and the Percentage of Retained Users for each Period, indexed by Cohort Period Month.</p>

```python
# Create a weighted data frame and reset the index
weighted = unstacked.reset_index()

# Add a Total Subs column which sums up all of the subscribers within each Cohort Period.
weighted['Total_Subs'] = weighted.drop('CohortPeriod', axis=1).sum(axis=1)

# Add a "placeholder" Retention Pct column; will calculate weighted averages within this column after adding.
weighted['Ret_Pct'] = weighted.iloc[:, 1:-1].sum(axis=1) / weighted.iloc[0, 1:-1].sum()

# These will modify all of the Retention Pcts to reflect the total duration of each cohort;
# This is probably better served with a function --> apply --> lamba expression approach
# In the future will figure out best way to scale this further.
weighted['Ret_Pct'].iloc[1] = weighted.iloc[1, 1:-3].sum() / weighted.iloc[0, 1:-3].sum()
weighted['Ret_Pct'].iloc[2] = weighted.iloc[2, 1:-4].sum() / weighted.iloc[0, 1:-4].sum()
weighted['Ret_Pct'].iloc[3] = weighted.iloc[3, 1:-5].sum() / weighted.iloc[0, 1:-5].sum()
weighted['Ret_Pct'].iloc[4] = weighted.iloc[4, 1:-6].sum() / weighted.iloc[0, 1:-6].sum()
weighted['Ret_Pct'].iloc[5] = weighted.iloc[5, 1:-7].sum() / weighted.iloc[0, 1:-7].sum()
weighted['Ret_Pct'].iloc[6] = weighted.iloc[6, 1:-8].sum() / weighted.iloc[0, 1:-8].sum()
weighted['Ret_Pct'].iloc[7] = weighted.iloc[7, 1:-9].sum() / weighted.iloc[0, 1:-9].sum()
weighted['Ret_Pct'].iloc[8] = weighted.iloc[8, 1:-10].sum() / weighted.iloc[0, 1:-10].sum()
weighted['Ret_Pct'].iloc[9] = weighted.iloc[9, 1:-11].sum() / weighted.iloc[0, 1:-11].sum()
weighted['Ret_Pct'].iloc[10] = weighted.iloc[10, 1:-12].sum() / weighted.iloc[0, 1:-12].sum()
weighted['Ret_Pct'].iloc[11] = weighted.iloc[11, 1:-13].sum() / weighted.iloc[0, 1:-13].sum()
weighted['Ret_Pct'].iloc[12] = weighted.iloc[12, 1:-14].sum() / weighted.iloc[0, 1:-14].sum()
weighted['Ret_Pct'].iloc[13] = weighted.iloc[13, 1:-15].sum() / weighted.iloc[0, 1:-15].sum()
weighted['Ret_Pct'].iloc[14] = weighted.iloc[14, 1:-16].sum() / weighted.iloc[0, 1:-16].sum()

# Return the full weighted data frame
weighted
```

```python
# Grab only the Cohort Period and Ret Pct columns
weighted_avg = weighted.filter(items=['CohortPeriod', 'Ret_Pct'])

# Transpose the values to run across the row rather that column
weighted_avg_transpose = weighted_avg.transpose()

# Return the weighted average data frame
weighted_avg_transpose
```

<img src="/assets/weighted_average_transpose2.png" alt="Unstacked Cohorts Dataframe" height="200"  style="width: 100%">

<p>Across all of the Cohorts in the data set that Greg originally used, we had 757 total users.  As you can see from the data above, in the month after their initial purchase, ~33% (row 3, Cohort Period 2) were retained in the second month -- at FloSports, we would call this M1 retention since M0 is the initial payment month. </p>
    
<p><strong>There are certainly areas for improvement within my Python code, and I always welcome suggestions on improvements, but regardless this is a massive time saver and is much easier to check for mistakes and therefore much less error-prone.</strong></p>


<p>In the financial models which I build and collaborate on, we include a retention curve schedule worksheet, which we dynamically select based on the business case / vertical running through the model's different scenarios (pricing, offerings, et al).  We do this in order to be able to conduct all sorts of pro forma analyses and this schedule worksheet could have 10 - 20 different retention curves at a time.  Adding a new selection is as simple as re-running the above analysis, after reading in the source data, and then reading out the data and pasting that weighted average retention series into your model's worksheet.
</p>

<p>
<strong>Hopefully I have made the time savings Python affords in acquiring the data to be pasted into these retention curve worksheets to be rather compelling.</strong> I have found the scaleability of doing analysis such as this in Python, a language I'm continually trying to improve in, to be fairly remarkable.  
</p>

<p>
My general approach is to use a data gathering language, SQL in my case, and gather data for specific date ranges and /or combinations of plan offerings for our subscribers; we then cut our curves as it makes most sense for the business(es) we are evaluating.  To achieve this flexibility within Excel is remarkably difficult, if not impossible, and continuing to find ways to scale within Python has been extremely eye opening.
</p>

<p><strong>In <a href="https://kdboller.github.io/2017/07/23/scaling-business-operational-insights-with-python_part2.html" target="_blank">Part 2</a> of this Series, I'll discuss the use of this approach within Mode Analytics, which I believe offers even more potential for massive efficiency improvements and gains from increased data insights.</strong>
</p>

 


