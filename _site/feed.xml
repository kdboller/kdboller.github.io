<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/datainformed/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/datainformed/" rel="alternate" type="text/html" /><updated>2017-05-07T17:40:06-05:00</updated><id>http://localhost:4000/datainformed/</id><title type="html">Kevin Boller’s Data Informed Blog</title><subtitle>This is a blog created by a sports nut and product guy; my content is focused on building a data informed culture in sports,  media and technology focused startups.  Topics include experimentation, analytics, growth and (attempts at) optimization.
</subtitle><entry><title type="html">Building an Analytics Tech Stack</title><link href="http://localhost:4000/datainformed/2017/03/22/building-an-analytics-tech-stack.html" rel="alternate" type="text/html" title="Building an Analytics Tech Stack" /><published>2017-03-22T12:00:00-05:00</published><updated>2017-03-22T12:00:00-05:00</updated><id>http://localhost:4000/datainformed/2017/03/22/building-an-analytics-tech-stack</id><content type="html" xml:base="http://localhost:4000/datainformed/2017/03/22/building-an-analytics-tech-stack.html">&lt;p&gt;In an ongoing attempt to be helpful to and learn from others serving in similar job capacities, I am continuing to review my experiences in building out our data infrastructure over the past ~12 months and discuss the most helpful applications which currently sit within our analytics tech stack. 
In my most recent post, Data Informed Experiments, I discussed the difference between being “data informed” and being “data driven”. I also provided some perspective related to building a “data informed” organizational culture – my intent is to share what we have learned along the way and discuss the applications which enable us to take advantage of all of the work we’ve completed to-date.&lt;/p&gt;

&lt;p&gt;When I arrived at FloSports a little over a year ago, I joined as a Product Director of Subscription Revenue. The company had great momentum in a market, live sports streaming, which was and continues to experience remarkable growth. However, from a data and analytics standpoint, a data warehouse did not yet exist; reporting was somewhat fragmented and siloed among departments, often living in Google Sheets. Although financial and subscription reporting and KPI monitoring was in place, it was clear that the analytical foundation would have a very challenging time scaling alongside the company’s significant growth trajectory. Among other aspects of our business model, in just a year and a half we’ve grown from producing and selling subscriptions against live events for 5 sports to over 20 sports, and this continues to grow.&lt;/p&gt;

&lt;p&gt;For business intelligence (BI), most companies consolidate their key data sources into a centralized data warehouse (DW); the process of extracting data from various sources and loading it into the DW is referred to as ETL; for reporting purposes, a business intelligence / reporting application is connected to the data warehouse, allowing for dashboards to be created with reports that leverage one or more of the DW’s data sources. While the build out of a DW is time consuming and can be challenging at times, the quick highlights of this process for us, which we kicked off less than a year ago, include the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Select where your data warehouse will reside / be hosted; like many other growth, tech companies, we selected Amazon Redshift&lt;/li&gt;
  &lt;li&gt;Identify your most critical data sources and build a roadmap for data source expansion&lt;/li&gt;
  &lt;li&gt;Evaluate build versus buy options along the way for key elements of the warehouse; I’ll discuss Segment in more detail later in this post&lt;/li&gt;
  &lt;li&gt;Outline and document the business rules and logic which will define how data is modeled and how the facts about your data will be reported on&lt;/li&gt;
  &lt;li&gt;Model all data and ensure that you have a ‘source of truth’ which you will use to validate each source loaded into your data warehouse&lt;/li&gt;
  &lt;li&gt;Choose BI visualization and ETL tools which best suit your organization’s requirements, budget and your team’s overall technical sophistication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the remainder of this post, I’ll highlight a few important tools which we’ve made use of since the second half of last year. These applications are incredibly important to every aspect of the analysis that we conduct and report on – in combination with our data warehouse, they’ve empowered me to be able to leave the world of “CSV extract, read into Jupyter Notebook, extract back to CSV, paste into financial model, and output to powerpoint”; I truly could not be more thankful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Periscope Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I was aware of Periscope Data while we were in the process of standing up our data warehouse; however, this post on Medium by Samsun Hu provided tremendous insight into the benefits of Periscope relative to more traditional, and generally costlier, BI tools.&lt;/p&gt;

&lt;p&gt;At FloSports, we use Periscope as our ‘consumer facing’ dashboard product – we have permissioned our dashboards by Groups and placed our business users into the Groups which align with their reporting needs and use cases. Here are my primary observations:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Filters: these allow for highly interactive dashboards, where end users can filter on different sports and date ranges to manipulate the returns in the various queries; this assists with scalability -- we can build a single dashboard which can be cut a number of different ways and on demand without data team involvement&lt;/li&gt;
  &lt;li&gt;Views: Views can be added, via directly loading SQL or CSV files, bypassing the need for ETL and enabling quick proof-of-concept dashboards which allow for demos of new data sources / data modeling&lt;/li&gt;
  &lt;li&gt;Data Cache: this ETLs customer databases into Periscope’s Redshift clusters, making queries highly performant and also enabling cross database joins, materialized views, and query-able CSV uploads&lt;/li&gt;
  &lt;li&gt;Outstanding customer support; data scientists are available via chat during business hours, oftentimes reviewing queries you are working on and helping you figure out how to write more complicated queries (or if you’re just stuck and need another set of eyes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Considerations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Cost: While Periscope Data is very easy to get going, pricing scales by every 1 billion rows; as one example on why this can be frustrating, we point our Staging database to Mode (see below) to avoid being bumped up to the next pricing tier&lt;/li&gt;
&lt;li&gt;Internal research / analytics; creating finalized dashboards for end user consumption is great; however, in terms of internal analysis, creating “burner” dashboards starts to become a maintenance headache&lt;/li&gt;
&lt;li&gt;For this reason, we prefer to use Mode for internal research / collaboration type work (also see below)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Mode Analytics&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mode Analytics markets its product as “a SQL editor, Python notebook, and visualization builder, all rolled into one.” After using it since November, I’m a huge, huge fan and I spend a significant amount of several days per week working in Mode.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Report builder concept; create a report, which includes a series of separate SQL queries and also allows for quick visualizations and an overall dashboard summary of data returns&lt;/li&gt;
  &lt;li&gt;Shareability of reports; similar in concept to Google Drive, share report links with other team members within Mode, who can then edit, add to, or clone your report for further analysis&lt;/li&gt;
  &lt;li&gt;We sometimes conduct analysis in Mode and export the dashboard as a PDF report which we send to business users&lt;/li&gt;
  &lt;li&gt;The ability to read data into a Python notebook; create a dataset via an SQL query and read it directly into the Python Notebook within the same application for more detailed analysis&lt;/li&gt;
  &lt;li&gt;Their blog and tutorials for SQL and Python are some of the best I’ve seen -- and you can create a Mode account and learn the application for FREE on public data sets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Considerations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dashboard features; while Mode is generally a great product, the one limitation is the ability to create interactive dashboards, e.g., filterable by attributes, which we make tremendous use of within Periscope Data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Segment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When building out a DW, there will always be a tradeoff between time spent engineering the ETL process / workflow(s) and creating reporting tables, e.g., aggregate fact tables, conducting machine learning, etc. The ETL serves as the plumbing of the DW and it’s obvious that high quality data is requisite to enabling key insights; as Bob Becker from the Kimball Group wrote: “At its foundation, the DW/BI environment must be a repository of high quality data. The biggest challenges for most DW/BI efforts are data-related. In general, the effort to develop the ETL processes required to provide high quality data is typically more demanding than anticipated” (my emphasis added).&lt;/p&gt;

&lt;p&gt;At FloSports, we are currently implementing Segment to serve as our customer data platform as well as the ETL-as-a-service for several of our data sources. We initially evaluated Segment’s Sources product earlier this year, which ETLs Zendesk, Salesforce, Stripe and a few other widely used cloud app sources. In the buy versus build scenario, the key question we were asking ourselves was whether or not we wanted to invest the time and resources in figuring out how to best ETL all of the data sources on our roadmap; or could we find a way to short circuit this? If we could bypass certain parts of the ETL, we would free up much more of our engineering time to generate valuable reporting which stitches multiple data sources together.&lt;/p&gt;

&lt;p&gt;My personal view is to always maintain focus and to drill on your core competency as much as possible – no off-the-shelf solution can define the logic which our business model dictates, and our user journey and combination of data sources are unique to our business. However, the process by which we ETL the data into a centralized repository does not need to be our core competency; after extensive evaluation, we determined that Segment was a very viable option which enables us to acquire several important, and widely used, data sources.&lt;/p&gt;

&lt;p&gt;Further, we want to warehouse all of our data across our applications and all 3rd party integrations and external data sources; and we really need a common identifier approach to tracking anonymous and logged in users across our various properties and platform. As a result, after extensive diligence we decided Segment’s customer data platform seemed to be the most time efficient and productive way to accomplish this, in combination with their Sources product.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cloud App Sources; the punchline here is this is ETL-as-a-service, point and click data source integration with your existing warehouse&lt;/li&gt;
  &lt;li&gt;200+ native integrations; Segment will send data generated within your apps to 3rd party services; similar in concept to a tag manager, except that Segment stores all data generated...forever&lt;/li&gt;
  &lt;li&gt;Warehouse connection; track data within your app and Segment will send the generated data to your connected data warehouse&lt;/li&gt;
  &lt;li&gt;Extensively helpful during diligence; we diligenced Segment for almost 2 months; our counterparts were knowledgeable and very helpful in assisting us with getting our heads around the benefits of a partnership&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Considerations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If you get the enterprise plan, this will be a material expense line item on your budget; I built a bottoms-up model that projected our Monthly Tracked Users (MTUs) for the next two years in order to discuss volume-based pricing&lt;/li&gt;
  &lt;li&gt;Nevertheless, you can either build this all in-house or leverage Segment for certain elements; there are tradeoffs with any SaaS product versus a build, but if we spend the majority of our time generating insights and drilling on our core competencies, we should generate the high ROI we anticipate&lt;/li&gt;
  &lt;li&gt;** You may want to discount my review here in general, as we are currently implementing Segment, whereas I’ve been working with Periscope and Mode for 6+ months&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This concludes my initial review of our journey in building out our data infrastructure over the past ~12 months and an overview of the most helpful applications which currently sit within our analytics tech stack. As our data warehouse product owner, I’ve taken a rather methodical approach in evaluating these applications, and we’ve found them to all be great products with excellent customer support and transparent visibility into their own product roadmaps. Over the course of the next year, we will be in the process of transitioning from smoothing out and expanding our analytics foundation to setting up our analytics stack to scale. I look forward to continuing to share our lessons learned and achieved victories along the way.&lt;/p&gt;

&lt;p&gt;If you have any feedback or insights into how you’ve built out and grown your analytics infrastructure, and / or your approach to BI, I would love to hear from you in the comments.&lt;/p&gt;</content><author><name></name></author><summary type="html">In an ongoing attempt to be helpful to and learn from others serving in similar job capacities, I am continuing to review my experiences in building out our data infrastructure over the past ~12 months and discuss the most helpful applications which currently sit within our analytics tech stack. In my most recent post, Data Informed Experiments, I discussed the difference between being “data informed” and being “data driven”. I also provided some perspective related to building a “data informed” organizational culture – my intent is to share what we have learned along the way and discuss the applications which enable us to take advantage of all of the work we’ve completed to-date.</summary></entry><entry><title type="html">Data Informed Experiments</title><link href="http://localhost:4000/datainformed/2017/02/27/data-informed-experiments.html" rel="alternate" type="text/html" title="Data Informed Experiments" /><published>2017-02-27T11:00:00-06:00</published><updated>2017-02-27T11:00:00-06:00</updated><id>http://localhost:4000/datainformed/2017/02/27/data-informed-experiments</id><content type="html" xml:base="http://localhost:4000/datainformed/2017/02/27/data-informed-experiments.html">&lt;p&gt;I am presently VP of Product (Revenue and Analytics) at FloSports, a direct-to-consumer provider of live streaming and on-demand digital sports content. In this role, I focus on product innovations which can drive top-line growth while also overseeing analytics (business, product, and data, among others). Recently, our CEO Martin Floreani has spoken about the business intelligence platform we have been building out since last year, which is internally known as Neptune. His interview with Sports Techie, where he discusses the impact Neptune has had on the business, can be found here. I have been fortunate to be one of FloSports’ founding members of Neptune and am responsible for overseeing both our data source and business intelligence reporting roadmaps.&lt;/p&gt;

&lt;p&gt;My goal in this post is primarily to:&lt;/p&gt;

&lt;p&gt;Jump start contributing my learnings in this role, where so many others before me have given back and from whom I’ve learned a great amount
Assist with continuing the momentum of companies becoming “data informed” rather than “data driven”
Relay a hypothetical experiment and its related process which is similar to the framework and approach that we leverage at FloSports
First, what is the difference in being “data informed” versus “data driven”. There are several solid online resources which discuss this; for the sake of time I will quickly highlight an insightful one which I read very recently. In this Interana post, the author delineates between the two as the following:&lt;/p&gt;

&lt;p&gt;Data-driven: You let the data guide your decision-making process&lt;/p&gt;

&lt;p&gt;Data-informed: You let data act as a check on your intuition&lt;/p&gt;

&lt;p&gt;Essentially, when an organization says that it is “data driven”, one risk it could be introducing is the possibility of optimizing for local maximums; this can occur due to solely focusing on the quantitative aspects of the available data, potentially disregarding key, relevant elements of the company’s underlying business model, and this approach can even run the risk of ultimately leading to a poorly driven decision due to inherent data biases related to the data’s collection and analysis.&lt;/p&gt;

&lt;p&gt;It is this last aspect that I would like to focus on. Companies are constantly evaluating ways to expand the number of subscribers who make it through their activation funnel; one potential hypothesis to consider is whether or not a free trial offering would be beneficial to the business. Free trials are used by numerous online streaming companies, such as Netflix, Hulu, and WWE.&lt;/p&gt;

&lt;p&gt;When testing a free trial, everyone understands that we are evaluating if activations spike with the “try before you buy approach” relative to the traditional request for an upfront payment. Some of the key metrics that I believe would be important to monitor in order to determine this particular experiment’s success, which also must be adjusted / normalized, include:&lt;/p&gt;

&lt;p&gt;Upfront conversion rate: the % of visitors who start a subscription
Plan % mix: the % of all started subscriptions who select a certain plan option, which may fluctuate by experiment variant
Trial-to-pay conversion rate: while the trial may generate greater upfront conversions, one must convert the trial subs to pay after the trial period
M1 Retention: compare historical retention curves to free trial participant retention; we need to be cognizant of how the free trial also impacts the lifetime value of subscribers (in addition to the upfront metrics cited above)&lt;/p&gt;

&lt;p&gt;The final piece to this analysis could be which subscribers should be included in evaluating the overall results. Many digital streaming providers have subscribers who churn and then return at a later date; the industry term for these is “reactivations”. For this particular experiment, I would most likely focus on “new” subscribers. We want to expand the overall activations within our funnel. Reactivated subscribers are more familiar with the product offering, as former subscribers, and also have different objectives and most likely different retention curves than new subscribers.&lt;/p&gt;

&lt;p&gt;How could being solely “data driven” lead to the wrong conclusions for a business analyzing the results of this experiment?&lt;/p&gt;

&lt;p&gt;There are several ways in which I believe one could poorly design this experiment and the framework used to evaluate the results. First, all site visitors could be treated equally, which I would argue is introducing flaws into the experiment’s data. As previously mentioned, I would only extract the subscribers in the experiment who are “new” — this better reflects the impact that “try before you buy” has on a subscriber’s willingness to activate. Including reactivated subscribers in the equation, who are more familiar and could even be potentially looking to game the system, could also skew the data, e.g., really high conversion rates (but they’re familiar with the product and liked not having to pay their first x days back).&lt;/p&gt;

&lt;p&gt;Also, if you focus solely on upfront conversions, you might believe that you have something incredible on the line; however, the product needs to stand on its own and convince subscribers to pay post trial period. If the experiment yielded 2x the upfront conversions, but the company only converted 50% of those conversions post trial, you are not better off and could potentially be worse off overall.&lt;/p&gt;

&lt;p&gt;There are qualitative aspects to the free trial and fundamental business model realities which also need to be understood. As one example, from a customer service perspective, are we introducing confusion and frustration, or are we increasing overall subscriber satisfaction and appreciation? While the quantitative results show the impact on metrics, there are other areas of the business that could enhance or diminish the overall results of the experiment and the anticipated long-term impact for the business.&lt;/p&gt;

&lt;p&gt;Last, and potentially most critically, let’s say that we let the experiment run for 3 months. If we did better overall during that period, does that mean that we should provide a free trial across the board? Not necessarily. We would want to evaluate observations at various times during the experiment’s duration to see if there were fluctuations in the results and, if present, understand the underlying reason(s) for those fluctuations. “Nature may abhor a vacuum” but experiment results will be volatile, unpredictable, potentially noisy and interpretation will most likely be parts art and science.&lt;/p&gt;

&lt;p&gt;In summary, being data informed is incredibly powerful. The experiments that we run always have the potential to surprise us one way or another. Having a framework by which we collect, analyze and normalize data, as well as how we use this data to check our intuition, can definitely have a strong impact on driving growth. Our company culture has evolved to make sure that all opinions have data to support or refute that view, and we are constantly hypothesizing how we can advance our business and also collect additional data. Businesses are constantly evolving, seeking a competitive advantage, and the most critical questions to answer are the ones you are not even yet aware you should be asking.&lt;/p&gt;

&lt;p&gt;In future posts, I would like to continue to share my perspective on how to help evolve a “data informed” company culture, continue to dive into hypothetical experiment and analytical scenarios, and discuss in more detail the applications which sit within our data and analytics stack. If you found this post useful or have examples you would like to share, I would love to hear from you in the comments.&lt;/p&gt;

&lt;p&gt;Additional image citations:&lt;/p&gt;

&lt;p&gt;The risk of optimizing for local maximums; from Andrew Chen: “Know the difference between data-informed and versus data-driven”
Photo Credit: Optimizely from Optimizely’s blog. Stat Engine is a powerful tool which shows near real-time results and statistical significance of those results.
Featured picture from Wikimedia Commons. This image illustrates the maxim that “nature abhors a vacuum.” This is not necessarily the case with the consistency of experiment results across time.&lt;/p&gt;</content><author><name></name></author><summary type="html">I am presently VP of Product (Revenue and Analytics) at FloSports, a direct-to-consumer provider of live streaming and on-demand digital sports content. In this role, I focus on product innovations which can drive top-line growth while also overseeing analytics (business, product, and data, among others). Recently, our CEO Martin Floreani has spoken about the business intelligence platform we have been building out since last year, which is internally known as Neptune. His interview with Sports Techie, where he discusses the impact Neptune has had on the business, can be found here. I have been fortunate to be one of FloSports’ founding members of Neptune and am responsible for overseeing both our data source and business intelligence reporting roadmaps.</summary></entry></feed>